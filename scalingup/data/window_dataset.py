from __future__ import annotations
from glob import glob
import logging
import re
import pytorch3d as pt3d
import zarr
import typing
from typing import Any, Dict, List, Optional, Sequence, Set, Tuple, Union
import lmdb
from rich.progress import track
import numpy as np
import torch
from pydantic import dataclasses, validator

from scalingup.data.dataset import (
    ReplayBuffer,
    StateTensor,
    TensorDataClass,
)
from scalingup.utils.generic import AllowArbitraryTypes, limit_threads
from scalingup.utils.core import (
    PartialObservation,
    PointCloud,
    split_state_phrase,
)
from transforms3d import quaternions, affines, euler


@dataclasses.dataclass(frozen=True)
class PolicyWindowRolloutConfig:
    # all units here are in action frequency cycles
    prediction_horizon: int
    proprio_obs_horizon: int
    vision_obs_horizon: int
    action_horizon: int
    #   |v|                             vision_obs_horizon
    # |o|o|                             proprio_obs_horizon
    # | |a|a|a|a|a|a|a|a|               action_horizon
    # |p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| prediction_horizon

    @validator("vision_obs_horizon")
    @classmethod
    def vision_obs_horizon_is_less_than_proprio(cls, v: int, values: Dict[str, Any]):
        if v > values["proprio_obs_horizon"]:
            raise ValueError(
                f"vision_obs_horizon ({v}) must be less than"
                + f" proprio_obs_horizon ({values['proprio_obs_horizon']})"
            )
        return v


@dataclasses.dataclass(config=AllowArbitraryTypes, frozen=True)
class StateSequenceTensor(TensorDataClass):
    sequence: List[StateTensor]  # T x B x ...

    @property
    def dtype(self) -> torch.dtype:
        return self.sequence[0].dtype

    @property
    def device(self) -> torch.device:
        return self.sequence[0].device

    @property
    def batch_size(self) -> int:
        return self.sequence[0].batch_size

    @property
    def is_batched(self) -> bool:
        return self.sequence[0].is_batched

    def transform(self, matrix: np.ndarray) -> StateSequenceTensor:
        return StateSequenceTensor(
            sequence=[state.transform(matrix=matrix) for state in self.sequence]
        )

    @classmethod
    def from_obs_sequence(
        cls,
        control_sequence: List[PartialObservation],
        pad_left_count: int,
        pad_right_count: int,
        **kwargs,
    ) -> StateSequenceTensor:
        # control_sequence should end with the obs
        # to currently predict the action for
        state_sequence = StateSequenceTensor(
            sequence=[
                StateTensor.from_obs(partial_obs=partial_obs, **kwargs)
                for partial_obs in control_sequence
            ]
        )
        if pad_left_count > 0:
            state_sequence = state_sequence.pad_left(count=pad_left_count)
        if pad_right_count > 0:
            state_sequence = state_sequence.pad_right(count=pad_right_count)

        return state_sequence

    def pad_left(self, count: int) -> StateSequenceTensor:
        return StateSequenceTensor(sequence=[self.sequence[0]] * count + self.sequence)

    def pad_right(self, count: int) -> StateSequenceTensor:
        return StateSequenceTensor(sequence=self.sequence + [self.sequence[-1]] * count)

    @classmethod
    def collate(cls, batch: Sequence[StateSequenceTensor]) -> StateSequenceTensor:
        assert all(
            len(state_sequence.sequence) == len(batch[0].sequence)
            for state_sequence in batch
        ), "Unequal window sizes"
        sequence_size = len(batch[0].sequence)
        attrs = list(StateTensor.__annotations__.keys())
        return StateSequenceTensor(
            sequence=[
                StateTensor(
                    **torch.utils.data.default_collate(  # type: ignore
                        [
                            {
                                attr: getattr(state_sequence.sequence[i], attr)
                                for attr in attrs
                            }
                            for state_sequence in batch
                        ]
                    )
                )
                for i in range(sequence_size)
            ],
        )

    def to(
        self,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
        non_blocking: bool = True,
    ) -> StateSequenceTensor:
        return StateSequenceTensor(
            sequence=[
                state.to(device=device, dtype=dtype, non_blocking=non_blocking)
                for state in self.sequence
            ],
        )

    def get_proprioception_data(self, proprioception_keys: Set[str]) -> torch.Tensor:
        return torch.stack(
            [
                state.get_proprioception_data(proprioception_keys)
                for state in self.sequence
            ],
            dim=-2,
        )

    @property
    def input_xyz_pts(self) -> torch.Tensor:
        return torch.stack(
            [state.input_xyz_pts for state in self.sequence],
            dim=-3,
        )

    @property
    def input_rgb_pts(self) -> torch.Tensor:
        return torch.stack(
            [state.input_rgb_pts for state in self.sequence],
            dim=-3,
        )

    @property
    def views(self) -> Dict[str, torch.Tensor]:
        return {
            view: torch.stack(
                [state.views[view] for state in self.sequence],
                dim=-4,
            )
            for view in self.sequence[0].views.keys()
        }


@dataclasses.dataclass(config=AllowArbitraryTypes, frozen=True)
class ControlActionTensor(TensorDataClass):
    value: torch.Tensor

    @property
    def dtype(self) -> torch.dtype:
        return self.value.dtype

    @property
    def device(self) -> torch.device:
        return self.value.device

    def to(
        self,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
        non_blocking: bool = True,
    ) -> ControlActionTensor:
        return typing.cast(
            ControlActionTensor,
            super().to(device=device, dtype=dtype, non_blocking=non_blocking),
        )


@dataclasses.dataclass(config=AllowArbitraryTypes, frozen=True)
class ControlActionSequenceTensor(TensorDataClass):
    sequence: List[ControlActionTensor]  # T x B x ...

    @property
    def dtype(self) -> torch.dtype:
        return self.sequence[0].dtype

    @property
    def device(self) -> torch.device:
        return self.sequence[0].device

    @property
    def tensor(self) -> torch.Tensor:
        return torch.stack([action.value for action in self.sequence], dim=-2)

    def to(
        self,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
        non_blocking: bool = True,
    ) -> ControlActionSequenceTensor:
        return ControlActionSequenceTensor(
            sequence=[
                action.to(device=device, dtype=dtype, non_blocking=non_blocking)
                for action in self.sequence
            ]
        )

    @classmethod
    def collate(
        cls, batch: Sequence[ControlActionSequenceTensor]
    ) -> ControlActionSequenceTensor:
        assert all(
            len(action_sequence.sequence) == len(batch[0].sequence)
            for action_sequence in batch
        ), "Unequal window sizes"
        sequence_size = len(batch[0].sequence)
        return ControlActionSequenceTensor(
            sequence=[
                ControlActionTensor(
                    value=torch.stack(
                        [action_sequence.sequence[i].value for action_sequence in batch]
                    )
                )
                for i in range(sequence_size)
            ],
        )

    @classmethod
    def from_control_sequence(
        cls,
        control_sequence: torch.Tensor,
        pad_left_count: int,
        pad_right_count: int,
    ) -> ControlActionSequenceTensor:
        # observation_sequence should end with the obs to currently
        # predict for
        action_sequence = ControlActionSequenceTensor(
            sequence=[ControlActionTensor(value=control) for control in control_sequence]
        )
        if pad_left_count > 0:
            action_sequence = action_sequence.pad_left(count=pad_left_count)
        if pad_right_count > 0:
            action_sequence = action_sequence.pad_right(count=pad_right_count)

        return action_sequence

    def pad_left(self, count: int) -> ControlActionSequenceTensor:
        return ControlActionSequenceTensor(
            sequence=[self.sequence[0]] * count + self.sequence
        )

    def pad_right(self, count: int) -> ControlActionSequenceTensor:
        return ControlActionSequenceTensor(
            sequence=self.sequence + [self.sequence[-1]] * count
        )


@dataclasses.dataclass(config=AllowArbitraryTypes, frozen=True)
class TrajectoryWindowTensor(TensorDataClass):
    state_sequence: StateSequenceTensor
    action_sequence: ControlActionSequenceTensor

    task_metrics: torch.Tensor  # T x B x ... (success, dense_success, etc.)
    task_names: List[str]  # B

    @property
    def dtype(self) -> torch.dtype:
        return self.state_sequence.dtype

    @property
    def device(self) -> torch.device:
        return self.state_sequence.device

    def to(
        self,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
        non_blocking: bool = True,
    ) -> TrajectoryWindowTensor:
        return TrajectoryWindowTensor(
            state_sequence=self.state_sequence.to(
                device=device, dtype=dtype, non_blocking=non_blocking
            ),
            action_sequence=self.action_sequence.to(
                device=device, dtype=dtype, non_blocking=non_blocking
            ),
            task_metrics=self.task_metrics.to(
                device=device, dtype=dtype, non_blocking=non_blocking
            ),
            task_names=self.task_names,
        )

    @classmethod
    def collate(cls, batch: Sequence[TrajectoryWindowTensor]) -> TrajectoryWindowTensor:
        assert all(
            traj_window_tensor.window_size == batch[0].window_size
            for traj_window_tensor in batch
        ), "Unequal window sizes"
        return TrajectoryWindowTensor(
            state_sequence=StateSequenceTensor.collate(
                [traj_window_tensor.state_sequence for traj_window_tensor in batch]
            ),
            action_sequence=ControlActionSequenceTensor.collate(
                [traj_window_tensor.action_sequence for traj_window_tensor in batch]
            ),
            task_metrics=torch.utils.data.default_collate(  # type: ignore
                [traj_step.task_metrics for traj_step in batch]
            ),
            task_names=sum((traj_step.task_names for traj_step in batch), start=[]),
        )

    @property
    def window_size(self) -> int:
        return len(self.state_sequence.sequence)

    def transform(self, matrix: np.ndarray) -> TrajectoryWindowTensor:
        return TrajectoryWindowTensor(
            state_sequence=self.state_sequence.transform(matrix=matrix),
            action_sequence=self.action_sequence,
            task_metrics=self.task_metrics,
            task_names=self.task_names,
        )

    @property
    def batch_size(self) -> int:
        return self.state_sequence.batch_size

    @property
    def is_batched(self) -> bool:
        return self.state_sequence.is_batched


@dataclasses.dataclass(config=AllowArbitraryTypes, frozen=True)
class RLTrajectoryWindowTensor(TensorDataClass):
    state_sequence: StateSequenceTensor
    action_sequence: ControlActionSequenceTensor

    next_state_sequence: StateSequenceTensor  # 下一个状态序列 StateSequenceTensor, 如果时最后的状态，那么 next 其实就是复制，不过计算 target q
    next_action_sequence: ControlActionSequenceTensor  # 下一个时刻的 history action sequence, 用于计算 target q
    # 不会用到，因为是根据 terminated 控制的。
    reward: torch.Tensor  # 奖励，最末尾的状态动作对才有，1或0
    terminated: torch.Tensor  # 是否终止，最末尾的状态动作对才有，1或0

    task_metrics: torch.Tensor  # T x B x ... (success, dense_success, etc.)
    task_names: List[str]  # B

    @property
    def dtype(self) -> torch.dtype:
        return self.state_sequence.dtype

    @property
    def device(self) -> torch.device:
        return self.state_sequence.device

    def to(
        self,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
        non_blocking: bool = True,
    ) -> RLTrajectoryWindowTensor:
        return RLTrajectoryWindowTensor(
            state_sequence=self.state_sequence.to(
                device=device, dtype=dtype, non_blocking=non_blocking
            ),
            action_sequence=self.action_sequence.to(
                device=device, dtype=dtype, non_blocking=non_blocking
            ),
            task_metrics=self.task_metrics.to(
                device=device, dtype=dtype, non_blocking=non_blocking
            ),
            task_names=self.task_names,
            # TODO: need to check
            next_state_sequence=self.next_state_sequence.to(
                device=device, dtype=dtype, non_blocking=non_blocking
            ),
            next_action_sequence=self.next_action_sequence.to(
                device=device, dtype=dtype, non_blocking=non_blocking
            ),
            reward=self.reward.to(
                device=device, dtype=dtype, non_blocking=non_blocking
            ),
            terminated=self.terminated.to(
                device=device, dtype=dtype, non_blocking=non_blocking
            ),
        )

    @classmethod
    def collate(cls, batch: Sequence[RLTrajectoryWindowTensor]) -> RLTrajectoryWindowTensor:
        assert all(
            traj_window_tensor.window_size == batch[0].window_size
            for traj_window_tensor in batch
        ), "Unequal window sizes"
        return RLTrajectoryWindowTensor(
            state_sequence=StateSequenceTensor.collate(
                [traj_window_tensor.state_sequence for traj_window_tensor in batch]
            ),
            action_sequence=ControlActionSequenceTensor.collate(
                [traj_window_tensor.action_sequence for traj_window_tensor in batch]
            ),
            task_metrics=torch.utils.data.default_collate(  # type: ignore
                [traj_step.task_metrics for traj_step in batch]
            ),
            task_names=sum((traj_step.task_names for traj_step in batch), start=[]),
            # TODO: need to check
            next_state_sequence=StateSequenceTensor.collate(
                [traj_window_tensor.next_state_sequence for traj_window_tensor in batch]
            ),
            next_action_sequence=ControlActionSequenceTensor.collate(
                [traj_window_tensor.next_action_sequence for traj_window_tensor in batch]
            ),
            reward=torch.stack(
                [traj_window_tensor.reward for traj_window_tensor in batch]
            ),
            terminated=torch.stack(
                [traj_window_tensor.terminated for traj_window_tensor in batch]
            ),
        )

    @property
    def window_size(self) -> int:
        return len(self.state_sequence.sequence)

    def transform(self, matrix: np.ndarray) -> RLTrajectoryWindowTensor:
        return RLTrajectoryWindowTensor(
            state_sequence=self.state_sequence.transform(matrix=matrix),
            action_sequence=self.action_sequence,
            task_metrics=self.task_metrics,
            task_names=self.task_names,

            next_state_sequence=self.next_state_sequence.transform(matrix=matrix),
            next_action_sequence=self.next_action_sequence,
            reward=self.reward,
            terminated=self.terminated,
        )

    @property
    def batch_size(self) -> int:
        return self.state_sequence.batch_size

    @property
    def is_batched(self) -> bool:
        return self.state_sequence.is_batched


@dataclasses.dataclass(config=AllowArbitraryTypes)
class NormalizationConfig:

    """
    normalizes input data to range [-1,1]
    """

    max_value: torch.Tensor
    min_value: torch.Tensor

    def normalize(self, x: torch.Tensor) -> torch.Tensor:
        self.min_value = self.min_value.to(x.device, x.dtype)
        self.max_value = self.max_value.to(x.device, x.dtype)
        return (x - self.min_value) / (self.max_value - self.min_value) * 2 - 1

    def unnormalize(self, x: torch.Tensor) -> torch.Tensor:
        self.min_value = self.min_value.to(x.device, x.dtype)
        self.max_value = self.max_value.to(x.device, x.dtype)
        return (x + 1) / 2 * (self.max_value - self.min_value) + self.min_value

    def to(self, device: Union[torch.device, str], dtype: Optional[torch.dtype] = None):
        self.min_value = self.min_value.to(device, dtype)
        self.max_value = self.max_value.to(device, dtype)
        return self


def copy_from_store(
    src: zarr.Group,
    dest: zarr.Group,
    if_exists="replace",
    compressor: Any = -1,
) -> zarr.Group:
    for key in src.keys():
        value = src[key]
        # copy with recompression
        if type(value) == zarr.Array:
            n_copied, n_skipped, n_bytes_copied = zarr.copy(
                source=value,
                dest=dest,
                name=key,
                chunks=value.chunks,
                compressor=value.compressor if compressor == -1 else compressor,
                if_exists=if_exists,
            )
        elif type(value) == zarr.Group:
            copy_from_store(
                src=value,
                dest=dest.create_group(key),
                if_exists=if_exists,
                compressor=compressor,
            )
        else:
            raise NotImplementedError(f"Cannot copy {type(value)}")
    for attr, value in src.attrs.asdict().items():
        dest.attrs[attr] = value
    return dest


class TrajectoryWindowDataset(ReplayBuffer):
    def __init__(self, rollout_config: PolicyWindowRolloutConfig, collate_fn=None, **kwargs):
        self.rollout_config = rollout_config
        self.min_action_values: Optional[torch.Tensor] = None
        self.max_action_values: Optional[torch.Tensor] = None
        self.path_obs_idx_tuples: List[Tuple[str, int]] = []
        self.control_frequencies: List[float] = []
        self.path_info: Dict[str, Tuple[str, Union[bool, float]]] = {}
        if collate_fn is None:
            logging.info("Using TrajectoryWindowTensor collate_fn")
            super().__init__(collate_fn=TrajectoryWindowTensor.collate, **kwargs)
        else:
            logging.info(f"Using custom collate_fn: {collate_fn}")
            super().__init__(collate_fn=collate_fn, **kwargs)

    @property
    def control_frequency(self) -> int:
        if self.control_frequencies is None:
            raise ValueError("Action frequency statistics has not been computed")
        frequency = np.mean(self.control_frequencies)
        if not np.allclose(frequency, np.rint(frequency)):
            raise ValueError(f"Action frequency ({frequency}Hz) is not integer")
        return int(np.rint(frequency))

    def reset_index(self):
        super().reset_index()
        self.path_obs_idx_tuples.clear()
        self.control_frequencies.clear()
        self.path_info.clear()
        self.min_action_values = None
        self.max_action_values = None

    @property
    def action_norm_config(self) -> NormalizationConfig:
        if self.min_action_values is None or self.max_action_values is None:
            raise ValueError("Action normalization statistics has not been computed")
        return NormalizationConfig(
            max_value=self.max_action_values, min_value=self.min_action_values
        )

    # @profile
    def __getitem__(self, idx):
        limit_threads(n=1)
        path, obs_end_idx = self.path_obs_idx_tuples[idx]
        """
        NOTE: `readonly=True` to prevent this error?
        ```
        self.db = lmdb.open(path, **kwargs)
        lmdb.InvalidParameterError: mdb_txn_begin: Invalid argument
        ```
        """
        # Use consolidated metadata store to make listing keys more
        # efficient.
        # https://zarr.readthedocs.io/en/stable/api/convenience.html#zarr.convenience.consolidate_metadata

        with zarr.LMDBStore(
            path,
            subdir=False,
            lock=False,
            readonly=True,
            readahead=False,
            map_size=int(2**18),
        ) as store:
            root = zarr.group(store=store)
            length = root.attrs["length"]
            task_desc = root.attrs["task_desc"]
            task_metric = self.task_metrics[idx]
            """
            example:
                pred_start_idx (-2)
                |       obs_end_idx (2)
                |       |
                V       V
                |o|o|o|o|o|                        obs_horizon (5)
                |a|a|a|a|a|a|a|a|a|a|a|a|          prediction_horizon (17)
                    |0|1|2|3|4|5|6|                control_sequence (7)
                |-|-|                              seq_pad_left (2)
                                |-|-|-|          action_pad_right_count (3)
            """

            assert (
                obs_end_idx >= 0
                and obs_end_idx < length + self.rollout_config.proprio_obs_horizon
            )

            pred_start_idx = obs_end_idx - self.rollout_config.proprio_obs_horizon + 1
            pred_end_idx = pred_start_idx + self.rollout_config.prediction_horizon
            clipped_pred_start_idx = max(0, pred_start_idx)
            assert (
                clipped_pred_start_idx < length and clipped_pred_start_idx < pred_end_idx
            )

            transform_matrix = (
                self.transform_augmentation.get_transform(
                    numpy_random=np.random.RandomState(
                        torch.randint(low=0, high=int(2**20), size=(1,)).item()  # type: ignore
                    )
                )
                if self.transform_augmentation is not None
                else None
            )

            proprio_indices = []
            control_indices = []
            for i in range(clipped_pred_start_idx, pred_end_idx):
                if i >= length:
                    break
                if (
                    i
                    < pred_end_idx
                    - self.rollout_config.prediction_horizon
                    + self.rollout_config.proprio_obs_horizon
                ):
                    # beyond this index, won't need observations anyways
                    proprio_indices.append(i)
                control_indices.append(i)
            control_sequence = self.get_action(
                root["control"],
                slice(control_indices[0], control_indices[-1] + 1),
                transform_matrix=transform_matrix,
            )
            proprio_slice = slice(proprio_indices[0], proprio_indices[-1] + 1)
            # vision horizon will always be shorter than proprio horizon
            vision_indices = proprio_indices[-self.rollout_config.vision_obs_horizon :]
            vision_slice = slice(vision_indices[0], vision_indices[-1] + 1)
            assert len(control_sequence) > 0
            state_tensor_sequence: List[StateTensor] = []
            if self.return_point_clouds:
                chunk_num_pts: int = root["state_tensor/input_rgb_pts"].chunks[1]  # type: ignore
                num_pts = int(root["state_tensor/input_rgb_pts"].shape[1])  # type: ignore
                num_chunks: int = num_pts // chunk_num_pts  # type: ignore
                # multiply by 1.5 because some points will likely fall outside
                # the scene bounds
                num_subsample_pts = min(int(self.num_obs_pts * 1.5), num_pts)
                num_subsample_chunks: int = num_subsample_pts // chunk_num_pts  # type: ignore
                chunk_indices = np.random.choice(
                    num_chunks, num_subsample_chunks, replace=False
                ).astype(int)
                pts_indices = (
                    chunk_indices[None, :].repeat(chunk_num_pts, axis=0) * chunk_num_pts
                )
                pts_indices += np.arange(chunk_num_pts)[:, None]
                pts_indices = pts_indices.reshape(-1)
                pts_indices.sort(axis=0)
                assert len(pts_indices) == len(np.unique(pts_indices))
            else:
                pts_indices = slice(0, 1)
            obs_views = {}
            if self.return_images:
                for view_name in self.obs_cameras:
                    images = root[f"state_tensor/views/{view_name}"][vision_slice]
                    obs_views[view_name] = torch.from_numpy(images).permute(
                        0, 3, 1, 2
                    )  # t h w c -> t c h w, where t is trajectory timestep
                    # vision_slice is shorter than proprio slice, so pad left
                    obs_views[view_name] = torch.cat(
                        (
                            torch.zeros(
                                [
                                    len(proprio_indices)
                                    - self.rollout_config.vision_obs_horizon,
                                ]
                                + list(obs_views[view_name].shape[1:]),
                                dtype=obs_views[view_name].dtype,
                            ),
                            obs_views[view_name],
                        ),
                        dim=0,
                    )
                assert all(
                    len(view) == len(proprio_indices) for view in obs_views.values()
                )

            input_rgb_pts_history = root[
                "state_tensor/input_rgb_pts"
            ].get_orthogonal_selection(
                (vision_slice, pts_indices, slice(None))
            )  # type: ignore
            input_xyz_pts_history = root[
                "state_tensor/input_xyz_pts"
            ].get_orthogonal_selection(
                (vision_slice, pts_indices, slice(None))
            )  # type: ignore

            if self.return_occupancies:
                assert (
                    transform_matrix is None
                ), "Occupancy volume not supported with transform augmentation"
                occupancy_vol_history = root["state_tensor/occupancy_vol"][
                    vision_slice
                ]  # type: ignore
            else:
                # fake occupancy volume
                occupancy_vol_history = np.zeros(
                    (len(input_xyz_pts_history), 1), dtype=bool
                )

            # vision_slice is shorter than proprio slice, so pad left
            input_rgb_pts_history = np.concatenate(
                (
                    np.zeros(
                        [len(proprio_indices) - self.rollout_config.vision_obs_horizon]
                        + list(input_rgb_pts_history[0].shape),
                        dtype=input_rgb_pts_history.dtype,
                    ),
                    input_rgb_pts_history,
                ),
                axis=0,
            )
            input_xyz_pts_history = np.concatenate(
                (
                    np.zeros(
                        [len(proprio_indices) - self.rollout_config.vision_obs_horizon]
                        + list(input_xyz_pts_history[0].shape),
                        dtype=input_xyz_pts_history.dtype,
                    ),
                    input_xyz_pts_history,
                ),
                axis=0,
            )
            occupancy_vol_history = np.concatenate(
                (
                    np.zeros(
                        [len(proprio_indices) - self.rollout_config.vision_obs_horizon]
                        + list(occupancy_vol_history[0].shape),
                        dtype=occupancy_vol_history.dtype,
                    ),
                    occupancy_vol_history,
                ),
                axis=0,
            )
            assert len(input_rgb_pts_history) == len(input_xyz_pts_history) and len(
                input_rgb_pts_history
            ) == len(proprio_indices)
            for i, (
                end_effector_rot_mat,
                end_effector_position,
                gripper_command,
                input_rgb_pts,
                input_xyz_pts,
                occupancy_vol,
                t,
            ) in enumerate(
                zip(
                    root["state_tensor/end_effector_orientation"][proprio_slice],
                    root["state_tensor/end_effector_position"][proprio_slice],
                    root["state_tensor/gripper_command"][proprio_slice],
                    input_rgb_pts_history,
                    input_xyz_pts_history,
                    occupancy_vol_history,
                    root["state_tensor/time"][proprio_slice],
                )
            ):
                rgb_pts = input_rgb_pts
                xyz_pts = input_xyz_pts
                if self.pos_bounds is not None:
                    point_cloud = (
                        PointCloud(
                            xyz_pts=xyz_pts,
                            rgb_pts=rgb_pts,
                            segmentation_pts={},
                        )
                        .filter_bounds(bounds=self.pos_bounds)
                        .subsample(
                            num_pts=self.num_obs_pts,
                            numpy_random=np.random.RandomState(
                                torch.randint(
                                    low=0, high=int(2**20), size=(1,)
                                ).item()  # type: ignore
                            ),
                        )
                    )
                    rgb_pts = point_cloud.rgb_pts
                    xyz_pts = point_cloud.xyz_pts

                # use half precision to save memory
                state_tensor_sequence.append(
                    StateTensor(
                        end_effector_orientation=torch.from_numpy(
                            end_effector_rot_mat.reshape(
                                -1
                            )  # TODO support other rotation representations
                        ).half(),
                        end_effector_position=torch.from_numpy(
                            end_effector_position
                        ).half(),
                        gripper_command=torch.from_numpy(gripper_command).bool(),
                        input_rgb_pts=torch.from_numpy(rgb_pts).half() / 255.0,
                        input_xyz_pts=torch.from_numpy(xyz_pts).half(),
                        time=torch.from_numpy(t).half(),
                        occupancy_vol=torch.from_numpy(occupancy_vol),
                        views={
                            view_name: view[i] for view_name, view in obs_views.items()
                        }
                        if self.return_images
                        else {},
                    )
                )

            # to handle cases where obs_horizon extends beyond the start of the trajectory
            seq_pad_left = clipped_pred_start_idx - pred_start_idx
            obs_end_idx = -seq_pad_left + self.rollout_config.proprio_obs_horizon

            state_sequence = StateSequenceTensor(sequence=state_tensor_sequence).pad_left(
                seq_pad_left
            )
            if transform_matrix is not None:
                state_sequence = state_sequence.transform(transform_matrix)

            # NOTE, currently not filtering position bounds or subsampling points

            assert len(state_sequence.sequence) == self.rollout_config.proprio_obs_horizon
            action_pad_right_count = max(
                self.rollout_config.prediction_horizon
                - (len(control_sequence) + seq_pad_left),
                0,
            )
            action_sequence = ControlActionSequenceTensor.from_control_sequence(
                control_sequence=control_sequence,
                pad_left_count=seq_pad_left,
                pad_right_count=action_pad_right_count,
            )
            assert (
                len(action_sequence.sequence) == self.rollout_config.prediction_horizon
            ), (
                f"expected {self.rollout_config.prediction_horizon} actions,"
                + f" got {len(action_sequence.sequence)}"
            )
            return TrajectoryWindowTensor(
                state_sequence=state_sequence,
                action_sequence=action_sequence,
                task_metrics=torch.tensor([task_metric]),
                task_names=[task_desc],
            )

    # @profile
    def get_action(
        self,
        control_root,
        selector: Optional[slice] = None,
        transform_matrix: Optional[np.ndarray] = None,
    ):
        if selector is None:
            selector = slice(None)
        if self.control_representation == "joint":
            control_actions = torch.tensor(control_root["joint_pos"][selector])
        elif self.control_representation == "ee":
            ee_pos = np.array(control_root["ee_pos"][selector])
            ee_rotmat = np.array(control_root["ee_rotmat"][selector])
            if transform_matrix is not None:
                ee_poss = []
                ee_rotmats = []
                for pos, rot_mat in zip(ee_pos, ee_rotmat):
                    pos, rot_mat = affines.decompose(
                        transform_matrix @ affines.compose(T=pos, R=rot_mat, Z=np.ones(3))
                    )[:2]

                    ee_poss.append(pos)
                    ee_rotmats.append(rot_mat)
                ee_pos = np.stack(ee_poss)
                ee_rotmat = np.stack(ee_rotmats)
            if self.rotation_representation == "quat":
                ee_rot = np.stack(
                    [quaternions.mat2quat(rotmat).reshape(-1) for rotmat in ee_rotmat]
                )
            elif self.rotation_representation == "mat":
                ee_rot = ee_rotmat
            elif self.rotation_representation == "upper_mat":
                ee_rot = ee_rotmat[:, :2, :].reshape(len(ee_rotmat), 6)
            else:
                raise ValueError(
                    f"Unknown rotation representation {self.rotation_representation}"
                )
            control_actions = torch.tensor(
                np.concatenate(
                    [
                        ee_pos,
                        ee_rot.reshape(len(ee_rot), -1),
                        np.array(control_root["ee_gripper_comm"][selector][:, None]),
                    ],
                    axis=-1,
                )
            )
        else:
            raise ValueError(
                f"Unknown control representation {self.control_representation}"
            )
        return control_actions

    def reindex(self, pbar: bool = False, remote: bool = True) -> int:
        mdb_pattern = re.compile(r"\.mdb$")
        self.reset_index()
        keep_indicator = []
        paths = list(
            filter(
                mdb_pattern.findall,
                map(str, sorted(glob(self.rootdir + "/**/*", recursive=True))),
            )
        )
        if self.max_num_datapoints is not None and self.max_num_datapoints < len(paths):
            logging.info(
                f"Subsampling {len(paths)} trajectories to {self.max_num_datapoints}"
            )
            self.numpy_random.shuffle(paths)
            paths = paths[: self.max_num_datapoints]
        total = len(paths)
        if total == 0:
            logging.info(f'{self.rootdir} does not contain any ".mdb" files')
            return 0
        if pbar:
            paths = track(paths, description=f"Reindexing {self.rootdir}")
        for path in paths:
            with zarr.LMDBStore(
                path, subdir=False, lock=False, readonly=False, map_size=int(2**18)
            ) as store:
                try:
                    root = zarr.group(store=store)
                    before = len(self)
                    # as the last element to get dumped, access state tensor just to make sure
                    # file is complete
                    if any(
                        f"state_tensor/views/{view}" not in root
                        for view in self.obs_cameras
                    ):
                        continue
                    control_actions = self.get_action(root["control"])
                    min_action_value = torch.min(control_actions, dim=0).values
                    max_action_value = torch.max(control_actions, dim=0).values
                    self.reindex_helper(
                        data=(
                            root.attrs["success"],
                            root.attrs["perfect"],
                            root.attrs["subtrajectory_success_rate"],
                            root.attrs["is_inferred_task"],
                            root.attrs["control_frequency"],
                            root.attrs["length"],
                            root.attrs["task_desc"],
                            min_action_value,
                            max_action_value,
                        ),
                        path=path,
                    )
                    after = len(self)
                    keep_indicator.append(after > before)
                except lmdb.CorruptedError:
                    logging.error(path + " corrupted")
                except Exception as e:  # noqa: B902
                    logging.error(e)
        logging.info(
            f"Using {len(self)} points, from {sum(keep_indicator)} trajectories"
            + f" out of {total} ({(sum(keep_indicator)/total)*100:.01f}%)"
        )
        self.summarize()
        freq = self.control_frequencies
        if len(freq) > 0:
            logging.info(f"Control frequency: {np.mean(freq):.02f}±{np.std(freq):.02f}Hz")
        return len(self)

    def reindex_helper(
        self,
        data: Tuple[bool, bool, float, bool, int, int, str, torch.Tensor, torch.Tensor],
        path: str,
    ):
        (
            is_successful,
            is_perfect,
            subtrajectory_success_rate,
            is_inferred_task,
            control_frequency,
            length,
            task_desc,
            min_action_value,
            max_action_value,
        ) = data
        task_metric: float = 0.0
        if self.task_metric == "success":
            task_metric = float(is_successful)
        elif self.task_metric == "dense_success":
            # subtrajectory success doesn't include root task
            # this means at test time, as long as we condition
            # on a dense success that is higher than 0.5 we should
            # get robust behavior that eventually succeed
            task_metric = (
                float(subtrajectory_success_rate) * 0.5 + float(is_successful) * 0.5
            )
        elif self.task_metric == "perfect":
            task_metric = float(is_perfect and is_successful)
        else:
            raise ValueError(f"Unknown task metric {self.task_metric}")

        if self.filter_negatives and task_metric == 0.0:
            logging.debug(f"Reject {path}: not success")
            return
        elif self.filter_manually_designed_trajectories and not is_inferred_task:
            logging.debug(f"Reject {path}: not inferred task")
            return
        task_desc = (
            task_desc
            if not self.remove_with_statement
            else split_state_phrase(task_desc)[1]
        )
        if self.train_tasks is not None and task_desc not in self.train_tasks:
            logging.debug(f"Reject {path}: not in train tasks")
            return
        """
        TODO write unit test for this

        Diffusion Policy is a behavior cloning framework, which assumes
        all trajectories given to it are successful. It indexes its
        trajectories start from this
          |-|-|-|*|
                |o|o|o|o|o|o|o|o|o|o|  observation_sequence

        to this
                            |-|-|-|*|
                |o|o|o|o|o|o|o|o|o|o|  observation_sequence

        where |-|-|-|*| has length self.rollout_config.proprio_obs_horizon
        and * is the current observation.

        In the latter case, the observation sequence is then right padded
        to encourage the policy to stop at the end of the trajectory. This is done
        by repeating the last observation and action in the sequence
                            |-|-|-|*|
                |o|o|o|o|o|o|o|o|o|o|           observation_sequence
                |a|a|a|a|a|a|a|a|a|a|           action_sequence
                                   ^ this action will be repeated
        However, in our case, we have trajectories that are not successful. We do
        not want the policy to pause if it didn't finish the task. Therefore, we
        will index the trajectory up until the prediction horizon hits the last
        action. For positive trajectories, we handle identically to diffusion policy.
        For negative trajectories, we index from this
          |-|-|-|*|
                |o|o|o|o|o|o|o|o|o|o|  observation_sequence
        to this
                      |-|-|-|*|!|!|!|  < this has length of `prediction_horizon`
                |o|o|o|o|o|o|o|o|o|o|  observation_sequence
        where all ! marks future observations the policy has to predict the actions
        for, including the *.
        """
        self.control_frequencies.append(control_frequency)
        if task_metric == 1.0:
            # NOTE: even for dense_success, we'll handle the same way. We'll drop
            # a few data points to prioritize correctness
            self.path_obs_idx_tuples.extend((path, i) for i in range(length))
            self.task_metrics.extend([task_metric] * length)
            self.task_names.extend([task_desc] * length)
        else:
            if length < self.rollout_config.prediction_horizon:
                # this is a failed trajectory that's too short, and will get
                # padded anyways. We'll just drop it.
                return
            # these are failed trajectories with long enough length.
            length -= self.rollout_config.prediction_horizon
            self.path_obs_idx_tuples.extend((path, i) for i in range(length))
            self.task_metrics.extend([task_metric] * length)
            self.task_names.extend([task_desc] * length)

        if self.min_action_values is None or self.max_action_values is None:
            self.min_action_values = min_action_value
            self.max_action_values = max_action_value
        else:
            self.min_action_values = torch.min(
                torch.cat(
                    [self.min_action_values[None, :], min_action_value[None, :]],
                    dim=0,
                ),
                dim=0,
            ).values
            self.max_action_values = torch.max(
                torch.cat(
                    [self.max_action_values[None, :], max_action_value[None, :]],
                    dim=0,
                ),
                dim=0,
            ).values
        self.path_info[path] = (task_desc, task_metric)


class RLTrajectoryWindowDataset(TrajectoryWindowDataset):
    def __init__(self, rollout_config: PolicyWindowRolloutConfig, **kwargs):
        # 调用父类的初始化方法，传递修改后的 collate_fn 参数
        super().__init__(rollout_config, collate_fn=RLTrajectoryWindowTensor.collate, **kwargs)
    def __getitem__(self, idx):
        print('collecting data, idx: ', idx)
        limit_threads(n=1)
        path, obs_end_idx = self.path_obs_idx_tuples[idx]
        # TODO: need to check
        # 由于我们一次是执行 self.rollout_config.action_horizon 个动作， 因此
        # 我们要加载下一个的状态序列是从 obs_end_idx + self.rollout_config.action_horizon 开始的
        next_obs_end_idx = obs_end_idx + self.rollout_config.action_horizon

        """
        NOTE: `readonly=True` to prevent this error?
        ```
        self.db = lmdb.open(path, **kwargs)
        lmdb.InvalidParameterError: mdb_txn_begin: Invalid argument
        ```
        """
        # Use consolidated metadata store to make listing keys more
        # efficient.
        # https://zarr.readthedocs.io/en/stable/api/convenience.html#zarr.convenience.consolidate_metadata

        with zarr.LMDBStore(
                path,
                subdir=False,
                lock=False,
                readonly=True,
                readahead=False,
                map_size=int(2 ** 18),
        ) as store:
            root = zarr.group(store=store)
            length = root.attrs["length"]
            task_desc = root.attrs["task_desc"]
            task_metric = self.task_metrics[idx]

            # 测试！！！！！！！！！！！！！！
            # 用于测试指定状态的 Q 值预测
            # obs_end_idx = length - 3 - self.rollout_config.action_horizon
            # obs_end_idx = 1
            # next_obs_end_idx = obs_end_idx + self.rollout_config.action_horizon

            """
            example:
                pred_start_idx (-2)
                |       obs_end_idx (2)
                |       |
                V       V
                |o|o|o|o|o|                        obs_horizon (5)
                |a|a|a|a|a|a|a|a|a|a|a|a|          prediction_horizon (17)
                    |0|1|2|3|4|5|6|                control_sequence (7)
                |-|-|                              seq_pad_left (2)
                                  |-|-|-|          action_pad_right_count (3)

                  |n|n|n|n|n|                      next_obs_horizon (5)
                  |-|                              next_seq_pad_left (1)   
            """
            # TODO: need to check
            state_sequence, action_sequence = self.get_state_sequence_and_action_sequence(obs_end_idx,
                                                                                          root,
                                                                                          length,
                                                                                          only_state_sequence=False)

            # 当 next_obs_end_idx 不是最后一个状态时，那么轨迹是没有结束的，因此 terminated = False
            # reward = 0
            if next_obs_end_idx < length - 1:
                next_state_sequence, next_action_sequence = self.get_state_sequence_and_action_sequence(
                    next_obs_end_idx,
                    root,
                    length,
                    only_state_sequence=False)

                terminated = torch.zeros(1, dtype=torch.bool)
                reward = torch.zeros(1, dtype=torch.float16)

            else:
                # 当 next_obs_end_idx 是最后一个状态或者超过最后一个状态时，那么轨迹是结束的，因此 terminated = True
                # 下一个状态序列 StateSequenceTensor, 如果时最后的状态，那么 next 其实就是复制，不过计算 target q
                # 不会用到，因为是根据 terminated 控制的。
                next_state_sequence, next_action_sequence = self.get_state_sequence_and_action_sequence(
                    length - 1,
                    root,
                    length,
                    only_state_sequence=False)
                terminated = torch.ones(1, dtype=torch.bool)
                reward = torch.tensor([task_metric], dtype=torch.float16)

            print('idx: ', idx, 'task_metric: ', task_metric, 'reward: ', reward)
            # 保证 reward 不是空
            assert reward.numel() > 0, "reward is empty"

            return RLTrajectoryWindowTensor(
                state_sequence=state_sequence,
                action_sequence=action_sequence,
                task_metrics=torch.tensor([task_metric]),
                task_names=[task_desc],

                next_state_sequence=next_state_sequence,  # (prediction_horizon, action_dim)
                next_action_sequence=next_action_sequence,
                reward=reward,
                terminated=terminated,
            )

    def get_state_sequence_and_action_sequence(self, obs_end_idx, root, length, only_state_sequence=False):
        """
            根据obs_end_idx，获取state_sequence和action_sequence
            example:
                pred_start_idx (-2)
                |       obs_end_idx (2)
                |       |
                V       V
                |o|o|o|o|o|                        obs_horizon (5)
                |a|a|a|a|a|a|a|a|a|a|a|a|          prediction_horizon (17)
                    |0|1|2|3|4|5|6|                control_sequence (7)
                |-|-|                              seq_pad_left (2)
                                  |-|-|-|          action_pad_right_count (3)

                  |n|n|n|n|n|                      next_obs_horizon (5)
                  |-|                              next_seq_pad_left (1)
        """
        # TODO: need to check
        assert (
                0 <= obs_end_idx < length + self.rollout_config.proprio_obs_horizon
        )

        pred_start_idx = obs_end_idx - self.rollout_config.proprio_obs_horizon + 1
        pred_end_idx = pred_start_idx + self.rollout_config.prediction_horizon
        clipped_pred_start_idx = max(0, pred_start_idx)
        assert (
                clipped_pred_start_idx < length and clipped_pred_start_idx < pred_end_idx
        )

        transform_matrix = (
            self.transform_augmentation.get_transform(
                numpy_random=np.random.RandomState(
                    torch.randint(low=0, high=int(2 ** 20), size=(1,)).item()  # type: ignore
                )
            )
            if self.transform_augmentation is not None
            else None
        )

        proprio_indices = []
        control_indices = []
        for i in range(clipped_pred_start_idx, pred_end_idx):  # 得到数据集中有的 propri 和 control 的索引
            if i >= length:
                break
            if (
                    i
                    < pred_end_idx
                    - self.rollout_config.prediction_horizon
                    + self.rollout_config.proprio_obs_horizon
            ):
                # beyond this index, won't need observations anyways
                proprio_indices.append(i)
            control_indices.append(i)
        control_sequence = self.get_action(
            root["control"],
            slice(control_indices[0], control_indices[-1] + 1),
            transform_matrix=transform_matrix,
        )
        proprio_slice = slice(proprio_indices[0], proprio_indices[-1] + 1)
        # vision horizon will always be shorter than proprio horizon
        vision_indices = proprio_indices[-self.rollout_config.vision_obs_horizon:]
        vision_slice = slice(vision_indices[0], vision_indices[-1] + 1)
        assert len(control_sequence) > 0
        state_tensor_sequence: List[StateTensor] = []
        if self.return_point_clouds:
            chunk_num_pts: int = root["state_tensor/input_rgb_pts"].chunks[1]  # type: ignore
            num_pts = int(root["state_tensor/input_rgb_pts"].shape[1])  # type: ignore
            num_chunks: int = num_pts // chunk_num_pts  # type: ignore
            # multiply by 1.5 because some points will likely fall outside
            # the scene bounds
            num_subsample_pts = min(int(self.num_obs_pts * 1.5), num_pts)
            num_subsample_chunks: int = num_subsample_pts // chunk_num_pts  # type: ignore
            chunk_indices = np.random.choice(
                num_chunks, num_subsample_chunks, replace=False
            ).astype(int)
            pts_indices = (
                    chunk_indices[None, :].repeat(chunk_num_pts, axis=0) * chunk_num_pts
            )
            pts_indices += np.arange(chunk_num_pts)[:, None]
            pts_indices = pts_indices.reshape(-1)
            pts_indices.sort(axis=0)
            assert len(pts_indices) == len(np.unique(pts_indices))
        else:
            pts_indices = slice(0, 1)
        obs_views = {}
        if self.return_images:
            for view_name in self.obs_cameras:
                images = root[f"state_tensor/views/{view_name}"][vision_slice]
                obs_views[view_name] = torch.from_numpy(images).permute(
                    0, 3, 1, 2
                )  # t h w c -> t c h w, where t is trajectory timestep
                # vision_slice is shorter than proprio slice, so pad left
                if self.rollout_config.proprio_obs_horizon > self.rollout_config.vision_obs_horizon:
                    obs_views[view_name] = torch.cat(
                        (
                            torch.zeros(
                                [
                                    len(proprio_indices)
                                    - self.rollout_config.vision_obs_horizon,
                                ]
                                + list(obs_views[view_name].shape[1:]),
                                dtype=obs_views[view_name].dtype,
                            ),
                            obs_views[view_name],
                        ),
                        dim=0,
                    )
            assert all(
                len(view) == len(proprio_indices) for view in obs_views.values()
            )

        input_rgb_pts_history = root[
            "state_tensor/input_rgb_pts"
        ].get_orthogonal_selection(
            (vision_slice, pts_indices, slice(None))
        )  # type: ignore
        input_xyz_pts_history = root[
            "state_tensor/input_xyz_pts"
        ].get_orthogonal_selection(
            (vision_slice, pts_indices, slice(None))
        )  # type: ignore

        if self.return_occupancies:
            assert (
                    transform_matrix is None
            ), "Occupancy volume not supported with transform augmentation"
            occupancy_vol_history = root["state_tensor/occupancy_vol"][
                vision_slice
            ]  # type: ignore
        else:
            # fake occupancy volume
            occupancy_vol_history = np.zeros(
                (len(input_xyz_pts_history), 1), dtype=bool
            )

        # vision_slice is shorter than proprio slice, so pad left
        if self.rollout_config.proprio_obs_horizon > self.rollout_config.vision_obs_horizon:
            input_rgb_pts_history = np.concatenate(
                (
                    np.zeros(
                        [len(proprio_indices) - self.rollout_config.vision_obs_horizon]
                        + list(input_rgb_pts_history[0].shape),
                        dtype=input_rgb_pts_history.dtype,
                    ),
                    input_rgb_pts_history,
                ),
                axis=0,
            )
            input_xyz_pts_history = np.concatenate(
                (
                    np.zeros(
                        [len(proprio_indices) - self.rollout_config.vision_obs_horizon]
                        + list(input_xyz_pts_history[0].shape),
                        dtype=input_xyz_pts_history.dtype,
                    ),
                    input_xyz_pts_history,
                ),
                axis=0,
            )
            occupancy_vol_history = np.concatenate(
                (
                    np.zeros(
                        [len(proprio_indices) - self.rollout_config.vision_obs_horizon]
                        + list(occupancy_vol_history[0].shape),
                        dtype=occupancy_vol_history.dtype,
                    ),
                    occupancy_vol_history,
                ),
                axis=0,
            )
        assert len(input_rgb_pts_history) == len(input_xyz_pts_history) and len(
            input_rgb_pts_history
        ) == len(proprio_indices)
        for i, (
                end_effector_rot_mat,
                end_effector_position,
                gripper_command,
                input_rgb_pts,
                input_xyz_pts,
                occupancy_vol,
                t,
        ) in enumerate(
            zip(
                root["state_tensor/end_effector_orientation"][proprio_slice],
                root["state_tensor/end_effector_position"][proprio_slice],
                root["state_tensor/gripper_command"][proprio_slice],
                input_rgb_pts_history,
                input_xyz_pts_history,
                occupancy_vol_history,
                root["state_tensor/time"][proprio_slice],
            )
        ):
            rgb_pts = input_rgb_pts
            xyz_pts = input_xyz_pts
            if self.pos_bounds is not None:
                point_cloud = (
                    PointCloud(
                        xyz_pts=xyz_pts,
                        rgb_pts=rgb_pts,
                        segmentation_pts={},
                    )
                    .filter_bounds(bounds=self.pos_bounds)
                    .subsample(
                        num_pts=self.num_obs_pts,
                        numpy_random=np.random.RandomState(
                            torch.randint(
                                low=0, high=int(2 ** 20), size=(1,)
                            ).item()  # type: ignore
                        ),
                    )
                )
                rgb_pts = point_cloud.rgb_pts
                xyz_pts = point_cloud.xyz_pts

            # use half precision to save memory
            state_tensor_sequence.append(
                StateTensor(
                    end_effector_orientation=torch.from_numpy(
                        end_effector_rot_mat.reshape(
                            -1
                        )  # TODO support other rotation representations
                    ).half(),
                    end_effector_position=torch.from_numpy(
                        end_effector_position
                    ).half(),
                    gripper_command=torch.from_numpy(gripper_command).bool(),
                    input_rgb_pts=torch.from_numpy(rgb_pts).half() / 255.0,
                    input_xyz_pts=torch.from_numpy(xyz_pts).half(),
                    time=torch.from_numpy(t).half(),
                    occupancy_vol=torch.from_numpy(occupancy_vol),
                    views={
                        view_name: view[i] for view_name, view in obs_views.items()
                    }
                    if self.return_images
                    else {},
                )
            )

        # to handle cases where obs_horizon extends beyond the start of the trajectory
        seq_pad_left = clipped_pred_start_idx - pred_start_idx
        obs_end_idx = -seq_pad_left + self.rollout_config.proprio_obs_horizon

        state_sequence = StateSequenceTensor(sequence=state_tensor_sequence).pad_left(
            seq_pad_left
        )
        if transform_matrix is not None:
            state_sequence = state_sequence.transform(transform_matrix)

        # NOTE, currently not filtering position bounds or subsampling points

        assert len(state_sequence.sequence) == self.rollout_config.proprio_obs_horizon
        action_pad_right_count = max(
            self.rollout_config.prediction_horizon
            - (len(control_sequence) + seq_pad_left),
            0,
        )
        action_sequence = ControlActionSequenceTensor.from_control_sequence(
            control_sequence=control_sequence,
            pad_left_count=seq_pad_left,
            pad_right_count=action_pad_right_count,
        )
        assert (
                len(action_sequence.sequence) == self.rollout_config.prediction_horizon
        ), (
                f"expected {self.rollout_config.prediction_horizon} actions,"
                + f" got {len(action_sequence.sequence)}"
        )

        if only_state_sequence:
            return state_sequence
        else:
            return state_sequence, action_sequence
